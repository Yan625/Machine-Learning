
# Long Short-Term Memory(LSTM)

1.Exploding/vanishing gradients are more problematic in the case of: RNNs

2.Effect of vanishing gradient problem with respect to initial input is:

    2.1)variations in initial input shows negligible effect on final output
    2.2)weights for initial inputs change negligibly

3.Vanishing gradient problem is faced when output is generated by passing input to a large number of hidden layers , why does this problem occurs in RNN where a single layer of RNN units is used:

    output of RNN layer is decided by backpropagating on all its initial values resulting in multiple weight updates
    
4.Gated Recurent Units-GRU

    4.1 The purpose of using GRU is:
        1) solve vanishing gradient problem in RNN
        2) enable memory unit to decide whethter to update.its memoty output for current input or not
        3) get a way to pass ai-1 as 1i without making any changes based on current input
     
    4.2 GRU units are faster to train as:
        1) they tend to work better on smaller dataset
        2) they involve less weights to train
    
    4.3 The two gates in a GRU are:
        1) reset
        2) update
    
    4.4 Work to determine how much of the past information is needed to be passed along future is done by:
        update gate
        
    4.5 Work to determine how much of the past information is to be forgotten is done by :
        reset gate
    
    4.6 Minimal gated unit uses equal weights and biases for reset and update gate .

5.LSTM:

    5.1 The purpose of not applying activation on memory cell is to :
        prevent the vanishing gradient as without activation weights are not changed drastically when back propagated through time
        
    5.2 The activation function usually used in the input gate of a LSTM is: sigmoid
    
    5.3 The purpose of output gate : to decide the extend to which Ct affect activation output


```python

```
