{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling\n",
    "\n",
    "    Feature Scaling is a data preprocessing technique. By preprocessing, we mean the transformations that are applied to the data before it is fed into some algorithm for some processing.\n",
    "    \n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "    <1.What is Feature Scaling?>\n",
    "\n",
    "    Feature Scaling is a technique where we standardize the range of all independent features of a data-set. It is also called Normalization. \n",
    "    \n",
    "    Generally, when we get raw data, all the features values varies on different scales. It is important to bring all the feature values on the scale so that value of one feature should not dominate over the others and hinder the performance of the learning algorithm. This process of bringing all the features values on the same scale is called feature scaling. Ensuring standardised feature values implicitly weights all features equally in their representation.\n",
    "    \n",
    "     Feature scaling or re-scaling of the features is performed such that they have the properties of normal distribution (most of the time) where values have standard deviation = 1, and mean = 0.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    <2.Why Feature Scaling is required?>\n",
    "    \n",
    "     Most of the real world applied machine learning algorithms are classification algorithms. Many of the classification algorithms works by calculating the distance between data points in space. If one feature has a wide range of values, then this feature is likely to dominate the distance measure between the data points over other features. Above this, if this feature proves to be insignificant in the end then, it will be hindering the algorithm results to a large extent. This will result in decrease in accuracy of the algorithm.\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    <3.How Feature Scaling is applied in sklearn?>\n",
    "    \n",
    "     There are many ways by which we can apply feature scaling on the dataset:\n",
    "        \n",
    "     1. The easiest way of scaling is to use - preprocessing.scale() function \n",
    "        A numpy array of values is given as input and output is numpy array with scaled values.This will \n",
    "        scale the values in such a way that mean of the values will be 0 and standard deviation will be 1.\n",
    "\n",
    "     2. Another method is to scale the features between given minimum and maximum values,generally between0and 1. \n",
    "        Function -preprocessing.MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "        Feature_range is given in the form of tuple - (min , max)\n",
    "        Copy - True (default), set it to False if you want inplace transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[ 1., -1.,  2.],\n",
    "              [ 2.,  0.,  0.],\n",
    "              [ 0.,  1., -1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.22474487,  1.33630621],\n",
       "       [ 1.22474487,  0.        , -0.26726124],\n",
       "       [-1.22474487,  1.22474487, -1.06904497]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled = preprocessing.scale(X)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled.std(axis= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.22474487,  1.33630621],\n",
       "       [ 1.22474487,  0.        , -0.26726124],\n",
       "       [-1.22474487,  1.22474487, -1.06904497]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = preprocessing.StandardScaler() # Transfer the parameter to X\n",
    "scaler.fit(X)\n",
    "scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  1.22474487, -0.26726124]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = [[1,1,0]]\n",
    "scaler.transform(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
