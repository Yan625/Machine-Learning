{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris=datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=iris.data\n",
    "y=iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier #MLP-->Multilayer perceptron多层感知器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=MLPClassifier() #classifier can take care for y_train even it is not [0,0,1] format\n",
    "clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9473684210526315"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hidden_layer_sizes=(100,)-->1 hidden layer with 100 units, (100,20,300) 3 hidden layers with 100,20,300 units each layer\n",
    "#activation='relu' defaut is the relu activation function\n",
    "#alpha=0.0001-->relariztion factor\n",
    "#learning_rate='constant'\n",
    "#batch_size='auto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(20,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_iter=3000, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=MLPClassifier(hidden_layer_sizes=(20,),max_iter=3000)\n",
    "clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9473684210526315"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-2.35446282e-02,  2.37022880e-01,  3.21450604e-02,\n",
       "          4.61109373e-31,  1.38198694e-01, -1.85559912e-01,\n",
       "         -6.97168784e-02, -1.62885050e-03,  1.83840811e-01,\n",
       "          2.51389933e-01, -1.50583442e-03, -7.66695253e-04,\n",
       "         -3.91197891e-07, -1.67218822e-05,  3.29077080e-01,\n",
       "         -2.17306604e-01, -6.61687630e-01, -1.59818470e-02,\n",
       "         -3.04829682e-03,  4.48696477e-28],\n",
       "        [-1.24340868e-02,  8.32064071e-01, -4.22078969e-01,\n",
       "         -1.74140092e-17, -3.76866820e-01,  1.52569349e-01,\n",
       "         -2.03731538e-02, -5.46022627e-08, -4.36413755e-01,\n",
       "          6.37386856e-01, -3.39291073e-05,  2.86203099e-04,\n",
       "          8.39506117e-07,  2.21149453e-30,  4.74262026e-01,\n",
       "          4.02062200e-01, -5.62967605e-01,  5.19943952e-01,\n",
       "          1.84669413e-05, -2.80438734e-03],\n",
       "        [-1.21627852e-02, -9.61513356e-01,  3.26611551e-02,\n",
       "         -2.85003337e-03, -8.86780624e-02,  2.37357126e-01,\n",
       "         -9.09977936e-02,  2.69760427e-31, -9.70642136e-02,\n",
       "         -5.77486877e-01, -4.21967434e-04,  4.04050299e-26,\n",
       "         -2.32219155e-31,  5.11896296e-13, -2.26038900e-01,\n",
       "         -1.76121748e-01,  1.18939597e+00,  8.61044133e-01,\n",
       "          1.00701171e-04, -2.20599898e-12],\n",
       "        [-3.97352500e-03, -7.15423251e-01,  2.86458173e-01,\n",
       "          2.98017362e-05,  2.15489526e-01, -1.40265200e-01,\n",
       "          2.68097168e-01, -5.76721132e-08,  2.67587960e-01,\n",
       "         -5.95435660e-01,  4.26989263e-06,  1.43733661e-22,\n",
       "          3.74599495e-10,  1.17713078e-03, -8.44228660e-01,\n",
       "         -2.63157961e-01,  1.33713199e+00,  6.24130043e-01,\n",
       "          2.89211280e-32,  2.92991473e-22]]),\n",
       " array([[-3.00444454e-07, -5.81182780e-04,  1.02079134e-03],\n",
       "        [ 1.00895911e+00, -1.19858434e+00, -7.41490536e-01],\n",
       "        [ 4.07873818e-01,  3.75432320e-01, -4.57557109e-01],\n",
       "        [-1.09466642e-03, -4.15408567e-06,  2.03425911e-03],\n",
       "        [-2.82182826e-01,  6.40341837e-02, -4.77463270e-01],\n",
       "        [ 3.70408549e-01,  2.33639161e-01, -2.26881560e-01],\n",
       "        [-4.40692666e-05,  4.57758868e-03, -2.11677647e-03],\n",
       "        [-2.74172597e-04,  4.96758529e-10, -9.55452017e-04],\n",
       "        [ 4.03129602e-01,  8.61378113e-02,  2.14377254e-01],\n",
       "        [ 7.91236945e-01, -5.38021018e-01, -1.43053760e+00],\n",
       "        [ 4.20760509e-07, -4.01345351e-05, -1.16554076e-19],\n",
       "        [-1.35396636e-03,  2.82719826e-03, -2.04597029e-03],\n",
       "        [ 4.67973999e-10,  5.19360364e-08, -7.09974922e-09],\n",
       "        [ 1.74900421e-32,  4.41399482e-07,  1.41520745e-29],\n",
       "        [ 2.28389450e-01,  8.07821394e-01, -5.31059994e-01],\n",
       "        [-4.27610298e-01,  1.96118240e-01,  3.36410309e-01],\n",
       "        [-8.99378977e-01, -1.32974559e+00,  1.19543053e+00],\n",
       "        [-7.51241210e-01,  1.85674403e-01,  1.01838929e-01],\n",
       "        [ 1.20618228e-08, -3.70557333e-08,  9.57057682e-33],\n",
       "        [-4.91499710e-09, -5.17205949e-08, -1.83018680e-03]])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#not include the relations of bias\n",
    "clf.coefs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2 layers\n",
    "len(clf.coefs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 20), (20, 3))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#f features and 20 layers, 20 layers in hidden and 3 layers in output\n",
    "clf.coefs_[0].shape,clf.coefs_[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20,), (3,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.intercepts_[0].shape,clf.intercepts_[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 without hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 2), (4, 1))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "Y = np.array([[0,0,0,1]]).T\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(z):\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivativeSig(x):\n",
    "    return sig(z)*(1 - sig(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [[0.40206599]\n",
      " [0.61405949]]\n",
      "\n",
      "2 [0.14840608]\n"
     ]
    }
   ],
   "source": [
    "# no hidden layer weights\n",
    "weights = 2* np.random.random((2, 1)) - 1 #(-1,1)\n",
    "bias = 2 * np.random.random(1) - 1        #(2,1) means m[m1,m2], only to one unit\n",
    "print(1,weights)\n",
    "print()\n",
    "print(2,bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.50062183],\n",
       "       [0.46011513],\n",
       "       [0.69513327],\n",
       "       [0.65967861]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward propagation without any hidden layer\n",
    "output = sig(np.dot(X, weights) + bias)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 one hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hidden layer weights\n",
    "wh = 2* np.random.random((2, 2)) - 1 #(2,1) means m[m1,m2], (2,2)two parameters to two units\n",
    "bh = 2* np.random.random((1, 2)) - 1 #(1,2) means one bias to two units\n",
    "wo = 2 * np.random.random((2, 1)) - 1\n",
    "bo = 2 * np.random.random((1, 1)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2558775 ],\n",
       "       [0.24275286],\n",
       "       [0.26029306],\n",
       "       [0.24631784]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward propagation with one hidden layer\n",
    "output0 = X\n",
    "outputHidden = sig(np.dot(output0, wh) + bh)\n",
    "output = sig(np.dot(outputHidden, wo) + bo)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Implementation of neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 without hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 2), (4, 1))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "Y = np.array([[0,0,0,1]]).T\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def derivativeSig(z):\n",
    "    return sig(z)*(1 - sig(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [[-0.435072  ]\n",
      " [-0.38781491]]\n",
      "\n",
      "2 [0.98160613]\n"
     ]
    }
   ],
   "source": [
    "# no hidden layer weights\n",
    "weights = 2* np.random.random((2, 1)) - 1\n",
    "bias = 2 * np.random.random(1) - 1\n",
    "lr = 0.1\n",
    "print(1,weights)\n",
    "print()\n",
    "print(2,bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [[10.36311698]\n",
      " [10.36311698]]\n",
      "\n",
      "2 [-5.71223875]\n",
      "\n",
      "3 [[0.00329438]\n",
      " [0.99053719]\n",
      " [0.99053719]\n",
      " [0.9999997 ]]\n"
     ]
    }
   ],
   "source": [
    "# forward propagation without any hidden layer\n",
    "for iter in range(10000):\n",
    "    output0 = X\n",
    "    output = sig(np.dot(output0, weights) + bias)\n",
    "\n",
    "    first_term = output - Y\n",
    "    input_for_last_layer = np.dot(output0, weights) + bias\n",
    "    second_term = derivativeSig(input_for_last_layer)\n",
    "    first_two = first_term * second_term\n",
    "    first_two.shape\n",
    "\n",
    "    changes = np.array([[0.0],[0.0]])\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(4):\n",
    "            changes[i][0] += first_two[j][0] * output0[j][i]\n",
    "    weights = weights - lr*changes\n",
    "    bias_change = 0.0\n",
    "    for j in range(4):\n",
    "        bias_change += first_two[j][0] * 1\n",
    "    bias = bias - lr * bias_change\n",
    "output = sig(np.dot(X, weights) + bias)\n",
    "print(1,weights) \n",
    "print()\n",
    "print(2,bias)\n",
    "print()\n",
    "print(3,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [[10.23971997]\n",
      " [10.23971997]]\n",
      "\n",
      "2 [-5.95054101]\n",
      "\n",
      "3 [[0.00259767]\n",
      " [0.98646941]\n",
      " [0.98646941]\n",
      " [0.99999951]]\n"
     ]
    }
   ],
   "source": [
    "# improvment\n",
    "# forward propagation without any hidden layer\n",
    "for iter in range(10000):\n",
    "    output0 = X\n",
    "    output = sig(np.dot(output0, weights) + bias)\n",
    "\n",
    "    first_term = output - Y\n",
    "    input_for_last_layer = np.dot(output0, weights) + bias\n",
    "    second_term = derivativeSig(input_for_last_layer)\n",
    "    first_two = first_term * second_term\n",
    "    first_two.shape\n",
    "\n",
    "    changes = np.dot(output0.T, first_two)\n",
    "    weights = weights - lr*changes\n",
    "    bias_change = np.sum(first_two)\n",
    "    bias = bias - lr * bias_change\n",
    "output = sig(np.dot(X, weights) + bias)\n",
    "\n",
    "print(1,weights)\n",
    "print()\n",
    "print(2,bias)\n",
    "print()\n",
    "print(3,output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 one hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 2), (4, 1))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "Y = np.array([[0,1,1,0]]).T\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hidden layer weights\n",
    "wh = 2* np.random.random((2, 2)) - 1 \n",
    "bh = 2* np.random.random((1, 2)) - 1 \n",
    "wo = 2 * np.random.random((2, 1)) - 1\n",
    "bo = 2 * np.random.random((1, 1)) - 1\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [[0.02553454]\n",
      " [0.49868719]\n",
      " [0.97723092]\n",
      " [0.50079372]]\n",
      "\n",
      "2 [[ 4.15241805 -3.3541149 ]\n",
      " [-8.18670281 -7.88967053]]\n",
      "\n",
      "3 [[-3.2164781   0.76281229]]\n",
      "\n",
      "4 [[ 5.78468692]\n",
      " [-5.66612072]]\n",
      "\n",
      "5 [[-0.00076821]]\n"
     ]
    }
   ],
   "source": [
    "for iter in range(10000):\n",
    "    output0 = X\n",
    "    inputHidden=np.dot(output0, wh) + bh\n",
    "    outputHidden = sig(inputHidden)\n",
    "    inputForOutputLayer=np.dot(outputHidden, wo) + bo\n",
    "    output = sig(inputForOutputLayer)\n",
    "\n",
    "    first_term_output_layer = output-Y\n",
    "    second_term_output_layer = derivativeSig(inputForOutputLayer)\n",
    "    first_two_output_layer = first_term_output_layer*second_term_output_layer\n",
    "\n",
    "    first_term_hidden_layer = np.dot(first_two_output_layer,wo.T)\n",
    "    second_term_hidden_layer = derivativeSig(inputHidden)\n",
    "    first_two_hidden_layer =  first_term_hidden_layer*second_term_hidden_layer\n",
    "\n",
    "    changes_output = np.dot(outputHidden.T,first_two_output_layer)\n",
    "    changes_output_bias = np.sum(first_two_output_layer,axis=0,keepdims=True)\n",
    "\n",
    "    changes_hidden = np.dot(output0.T,first_two_hidden_layer)\n",
    "    changes_hidden_bias = np.sum(first_two_hidden_layer,axis=0,keepdims=True)\n",
    "\n",
    "    wo=wo-lr*changes_output\n",
    "    bo=bo-lr*changes_output_bias\n",
    "\n",
    "    wh=wh-lr*changes_hidden\n",
    "    bh=bh-lr*changes_hidden_bias\n",
    "\n",
    "output0 = X\n",
    "inputHidden=np.dot(output0, wh) + bh\n",
    "outputHidden = sig(inputHidden)\n",
    "inputForOutputLayer=np.dot(outputHidden, wo) + bo\n",
    "output = sig(inputForOutputLayer)\n",
    "\n",
    "print(1,output)\n",
    "print()\n",
    "print(2,wh)\n",
    "print()\n",
    "print(3,bh)\n",
    "print()\n",
    "print(4,wo)\n",
    "print()\n",
    "print(5,bo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
